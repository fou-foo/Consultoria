}
}
d[is.na(d)] <- 0
mini <- min(d)
candidatos <- which(d==mini)
c.asig <- sample(candidatos, 1)#romper empates aleatoriamente
(1:K)[c.asig]#nueva asignacion de cluster para la observacion
}, X =1:dim(data)[1]#, mc.cores = detectCores()-2
)
data$cluster <- unlist(nuevo.cluster) #juntamos los resultados
}
return(data$cluster) #cluster finales
}
}
############################################################
set.seed(0)
#####simulacion de datos parecidos a los del paper mencionado
#####son dos circunferencias con centro (.5,.5) y radios 1 y 4
#####se agrega en cada eje ruido ~ N(0,sigma=1/10 ) y N(0,1/10)
r <- 1 #radio
n <- 100 #la cuarta parte del numero de puntos que se van a generar
#se genera la primer circunferencia con ruido
x <- seq(-r, r, length=n)
y1 <- sqrt(r**2-x**2) + rnorm(n,0,r/10)
y2 <- -sqrt(r**2-x**2) - rnorm(n,0,r/10)
m.a1 <- data.frame(x=rep(x+.5, 2), y = c(y1+.5,y2+.5), clase=1)
#se genera la segunda circunferencia con ruido
r <- 4
x <- seq(-r, r, length=n)
y1 <- sqrt(r**2-x**2) + rnorm(n,0,r/40)
y2 <- -sqrt(r**2-x**2) - rnorm(n,0,r/40)
m.a2 <- data.frame(x=rep(x+.5, 2), y = c(y1+.5,y2+.5), clase=2)
m.a <- rbind(m.a1, m.a2) #nuestro primer conjunto de prueba
m.a2 <- as.data.frame(scale(m.a[,1:2]) )
m.a[,1:2] <- m.a2
library(ggplot2)
ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(clase))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Muestra aleatoria generada (400 obs)') +xlab('') + ylab('')
set.seed(0)
label<- kmeans(m.a, centers = 2, nstart = 100) #comparamos el desempeÃ±o de kmeans en vista
#                              #de que apriori sabemos que son 2 grupos
p1 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(label$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kmeans (accuracy .49%)') +xlab('') + ylab('')
#visualizamos los resultados
#de clasificacion usando kmeans
sum(diag(as.matrix(table(m.a$clase, label$cluster))))/dim(m.a)[1] #se calcula el accuracy que es de .265, .735,.4975
#visualizamos los resultados
#de clasificacion usando kmeans
table(m.a$clase, label$cluster) #se calcula el accuracy que es de .265, .735,.4975
101+100
201/400
199/400
#                              #de que apriori sabemos que son 2 grupos
p1 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(label$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kmeans (accuracy 50%)') +xlab('') + ylab('')
#uso sobre
set.seed(0)
Kernel.kmeans.simu <- Kernel.kmeans.init(m.a[, 1:2])
Kernel.kmeans.simu <- Kernel.kmeans.init(m.a[, 1:2])
labels <- Kernel.kmeans.simu(data= m.a[, 1:2], sigma = -1, t = 20, k =2)
#uso sobre
set.seed(0)
Kernel.kmeans.simu <- Kernel.kmeans.init(m.a[, 1:2])
Kernel.kmeans.simu
############################################################
set.seed(0)
#####simulacion de datos parecidos a los del paper mencionado
#####son dos circunferencias con centro (.5,.5) y radios 1 y 4
#####se agrega en cada eje ruido ~ N(0,sigma=1/10 ) y N(0,1/10)
r <- 1 #radio
n <- 100 #la cuarta parte del numero de puntos que se van a generar
#se genera la primer circunferencia con ruido
x <- seq(-r, r, length=n)
y1 <- sqrt(r**2-x**2) + rnorm(n,0,r/10)
y2 <- -sqrt(r**2-x**2) - rnorm(n,0,r/10)
m.a1 <- data.frame(x=rep(x+.5, 2), y = c(y1+.5,y2+.5), clase=1)
#se genera la segunda circunferencia con ruido
r <- 4
x <- seq(-r, r, length=n)
y1 <- sqrt(r**2-x**2) + rnorm(n,0,r/40)
y2 <- -sqrt(r**2-x**2) - rnorm(n,0,r/40)
m.a2 <- data.frame(x=rep(x+.5, 2), y = c(y1+.5,y2+.5), clase=2)
m.a <- rbind(m.a1, m.a2) #nuestro primer conjunto de prueba
m.a2 <- as.data.frame(scale(m.a[,1:2]) )
m.a[,1:2] <- m.a2
library(ggplot2)
ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(clase))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Muestra aleatoria generada (400 obs)') +xlab('') + ylab('')
set.seed(0)
label<- kmeans(m.a, centers = 2, nstart = 100) #comparamos el desempeÃ±o de kmeans en vista
#                              #de que apriori sabemos que son 2 grupos
p1 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(label$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kmeans (accuracy 50%)') +xlab('') + ylab('')
Kernel.kmeans.simu <- Kernel.kmeans.init(m.a[, 1:2])
labels <- Kernel.kmeans.simu(data= m.a[, 1:2], sigma = -1, t = 20, k =2)
data= m.a[, 1:2]
sigma = -1
t = 20
k =2
#se calcula la matriz de distancias 'MK'
# ESTA FUNCION ES UN CLOSURE, REGRESA OTRA FUNCION, funciona como un constructor de clase del paradigma POO
# LA FINALIDAD ES CALCULUAR LA MATRIZ DE KERNEL UNA SOLA VEZ Y PROBAR DIFERENTES VALORES DE PARAMETROS
n <- dim(data)[1]
#comienza calculo de la matriz superior del kernel entre todos los pares de observaciones
MK <- matrix(rep(0, n*n), ncol = n)
data.matrix <- as.matrix(data)
p <- dim(data.matrix)[2]
MK <- CalculaKernel(MK, n, data.matrix, p)
#data (data.frame) con las observaciones a clasificar
#sigma (numeric): shift mencionado en el paper
#t (numeric): numero de iteraciones
#k (numeric): numero de clusters
data$cluster <- sample(1:k,dim(data)[1], replace = TRUE)#asignacion inicial aleatoria
for(x in 1:t)
{
#siguiendo el paper citado realizamos las iteraciones
#hacemos uso del shift
nuevo.cluster <- lapply(FUN=function(i)
{
#parte del calculo que no depende del shift
d <- rep(-1, k)
for(z in 1:k)
{
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
d[is.na(d)] <- 0 #para cachar los errores de underflow
}
pis <- data.frame(table(data$cluster))
# suma del shift a las entradas correspondientes
for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
d[is.na(d)] <- 0
mini <- min(d)
candidatos <- which(d==mini)
c.asig <- sample(candidatos, 1)#romper empates aleatoriamente
(1:K)[c.asig]#nueva asignacion de cluster para la observacion
}, X =1:dim(data)[1]#, mc.cores = detectCores()-2
)
data$cluster <- unlist(nuevo.cluster) #juntamos los resultados
}
do realizamos las iteraciones
#hacemos uso del shift
nuevo.cluster <- lapply(FUN=function(i)
{
#parte del calculo que no depende del shift
d <- rep(-1, k)
for(z in 1:k)
{
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
d[is.na(d)] <- 0 #para cachar los errores de underflow
}
pis <- data.frame(table(data$cluster))
# suma del shift a las entradas correspondientes
for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
d[is.na(d)] <- 0
mini <- min(d)
candidatos <- which(d==mini)
c.asig <- sample(candidatos, 1)#romper empates aleatoriamente
(1:K)[c.asig]#nueva asignacion de cluster para la observacion
}
)
do realizamos las iteraciones
#hacemos uso del shift
nuevo.cluster <- lapply(FUN=function(i)
{
#parte del calculo que no depende del shift
d <- rep(-1, k)
for(z in 1:k)
{
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
d[is.na(d)] <- 0 #para cachar los errores de underflow
}
pis <- data.frame(table(data$cluster))
# suma del shift a las entradas correspondientes
for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
d[is.na(d)] <- 0
mini <- min(d)
candidatos <- which(d==mini)
c.asig <- sample(candidatos, 1)#romper empates aleatoriamente
(1:K)[c.asig]#nueva asignacion de cluster para la observacion
}
)
#siguiendo el paper citado realizamos las iteraciones
#hacemos uso del shift
nuevo.cluster <- lapply(FUN=function(i){
#parte del calculo que no depende del shift
d <- rep(-1, k)
for(z in 1:k)
{
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
d[is.na(d)] <- 0 #para cachar los errores de underflow
}
pis <- data.frame(table(data$cluster))
# suma del shift a las entradas correspondientes
for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
d[is.na(d)] <- 0
mini <- min(d)
candidatos <- which(d==mini)
c.asig <- sample(candidatos, 1)#romper empates aleatoriamente
(1:K)[c.asig]#nueva asignacion de cluster para la observacion
}, X =1:dim(data)[1]#, mc.cores = detectCores()-2
)
for(x in 1:t)
{
#siguiendo el paper citado realizamos las iteraciones
#hacemos uso del shift
nuevo.cluster <- lapply(FUN=function(i){
#parte del calculo que no depende del shift
d <- rep(-1, k)
for(z in 1:k)
{
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
d[is.na(d)] <- 0 #para cachar los errores de underflow
}
pis <- data.frame(table(data$cluster))
# suma del shift a las entradas correspondientes
for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
d[is.na(d)] <- 0
mini <- min(d)
candidatos <- which(d==mini)
c.asig <- sample(candidatos, 1)#romper empates aleatoriamente
(1:k)[c.asig]#nueva asignacion de cluster para la observacion
}, X =1:dim(data)[1]#, mc.cores = detectCores()-2
)
data$cluster <- unlist(nuevo.cluster) #juntamos los resultados
}
library(Rcpp)
sourceCpp("C:\\Users\\fou-f\\Desktop\\MCE\\Second\\CienciaDeDatos\\tarea2\\kernel.cpp")
######### Implementacion de kernel k-means con shift basado en el paper:
#########Inderjit Dhillon, Yuqiang Guan and Brian Kulis.
#####A Unified view of Kernel k-means, Spectral Clustering and Graph Cuts.
Kernel.kmeans.init <- function(data )
{
# data (data.frame): data set con las observaciones y solo columnas numericas
#se calcula la matriz de distancias 'MK'
# ESTA FUNCION ES UN CLOSURE, REGRESA OTRA FUNCION, funciona como un constructor de clase del paradigma POO
# LA FINALIDAD ES CALCULUAR LA MATRIZ DE KERNEL UNA SOLA VEZ Y PROBAR DIFERENTES VALORES DE PARAMETROS
n <- dim(data)[1]
#comienza calculo de la matriz superior del kernel entre todos los pares de observaciones
MK <- matrix(rep(0, n*n), ncol = n)
data.matrix <- as.matrix(data)
p <- dim(data.matrix)[2]
MK <- CalculaKernel(MK, n, data.matrix, p)
#termina calculo de matriz de kernel
function(data, sigma,t, k )
{
#data (data.frame) con las observaciones a clasificar
#sigma (numeric): shift mencionado en el paper
#t (numeric): numero de iteraciones
#k (numeric): numero de clusters
data$cluster <- sample(1:k,dim(data)[1], replace = TRUE)#asignacion inicial aleatoria
for(x in 1:t)
{
#siguiendo el paper citado realizamos las iteraciones
#hacemos uso del shift
nuevo.cluster <- lapply(FUN=function(i){
#parte del calculo que no depende del shift
d <- rep(-1, k)
for(z in 1:k)
{
j <- which(data$cluster == z)
d[z]<- MK[i,i] - 2*sum(MK[i, j])/length(j) +
sum(MK[j,j])/(length(j))**2
d[is.na(d)] <- 0 #para cachar los errores de underflow
}
pis <- data.frame(table(data$cluster))
# suma del shift a las entradas correspondientes
for(z in 1:k)
{
if(z !=data[i,'cluster'])
{
d[z] <- d[z] + sigma +sigma/pis[z,'Freq']
}else{
d[z] <- d[z] + sigma -sigma/pis[data[i,'cluster'],'Freq']
}
}
d[is.na(d)] <- 0
mini <- min(d)
candidatos <- which(d==mini)
c.asig <- sample(candidatos, 1)#romper empates aleatoriamente
(1:k)[c.asig]#nueva asignacion de cluster para la observacion
}, X =1:dim(data)[1]#, mc.cores = detectCores()-2
)
data$cluster <- unlist(nuevo.cluster) #juntamos los resultados
}
return(data$cluster) #cluster finales
}
}
############################################################
set.seed(0)
#####simulacion de datos parecidos a los del paper mencionado
#####son dos circunferencias con centro (.5,.5) y radios 1 y 4
#####se agrega en cada eje ruido ~ N(0,sigma=1/10 ) y N(0,1/10)
r <- 1 #radio
n <- 100 #la cuarta parte del numero de puntos que se van a generar
#se genera la primer circunferencia con ruido
x <- seq(-r, r, length=n)
y1 <- sqrt(r**2-x**2) + rnorm(n,0,r/10)
y2 <- -sqrt(r**2-x**2) - rnorm(n,0,r/10)
m.a1 <- data.frame(x=rep(x+.5, 2), y = c(y1+.5,y2+.5), clase=1)
#se genera la segunda circunferencia con ruido
r <- 4
x <- seq(-r, r, length=n)
y1 <- sqrt(r**2-x**2) + rnorm(n,0,r/40)
y2 <- -sqrt(r**2-x**2) - rnorm(n,0,r/40)
m.a2 <- data.frame(x=rep(x+.5, 2), y = c(y1+.5,y2+.5), clase=2)
m.a <- rbind(m.a1, m.a2) #nuestro primer conjunto de prueba
m.a2 <- as.data.frame(scale(m.a[,1:2]) )
m.a[,1:2] <- m.a2
library(ggplot2)
ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(clase))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Muestra aleatoria generada (400 obs)') +xlab('') + ylab('')
set.seed(0)
label<- kmeans(m.a, centers = 2, nstart = 100) #comparamos el desempeÃ±o de kmeans en vista
#                              #de que apriori sabemos que son 2 grupos
p1 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(label$cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kmeans (accuracy 50%)') +xlab('') + ylab('')
#uso sobre
set.seed(0)
Kernel.kmeans.simu <- Kernel.kmeans.init(m.a[, 1:2])
labels <- Kernel.kmeans.simu(data= m.a[, 1:2], sigma = -1, t = 20, k =2)
m.a$cluster <- labels
p3 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kernel.kmeans (accuracy .72%)') +xlab('') + ylab('')
dim(m.a)[1]
table(m.a$clase,m.a$cluster)
99/400
p3
labels <- Kernel.kmeans.simu(data= m.a[, 1:2], sigma = -1, t = 100, k =2)
m.a$cluster <- labels
p3 <- ggplot(m.a,#visualizamos nuestro primer conjunto de prueba
aes(x=x, y=y, color = factor(cluster))) + geom_point() +
theme_minimal() + theme(legend.position='none') +
ggtitle('Agrupamiento de kernel.kmeans (accuracy .72%)') +xlab('') + ylab('')
p3
table(m.a$clase,m.a$cluster)
200+61
261/400
labels <- Kernel.kmeans.simu(data= m.a[, 1:2], sigma = -1, t = 1000, k =2)
colors()
?rgb
col2rgb(colors())
plot(1:10, pch=20, col =rainbow(n=10))
rainbow(n=10)
rainbow(n=16)
library(jsonlite)
salidaResultados <- 'C:\\Users\\fou-f\\Desktop\\Consultoria\\python'
setwd(salidaResultados)# nos cambiamos de directorio
out <- 'C:\\Users\\fou-f\\Desktop\\Consultoria\\python\\datosdemanda'
entrada <- 'C:\\Users\\fou-f\\Desktop\\Consultoria\\python\\datosdemanda.zip'
archivosLista <- dir(out) #los archivos con los cuales trabajaremos
archivosLista <- sort(archivosLista)
datos <- lapply(1:length(archivosLista), FUN = function(i){
pathArchivo <- paste0(out,'\\', archivosLista[i])
archivoNojason  = fromJSON(pathArchivo)
archivoNojason
})
data <- do.call("rbind", datos)
data$valorEnlace <- NULL
data <- apply(data, 2, function(x){
as.numeric(x)
})
data <- apply(data, 2, function(x){
as.numeric(x)
})
data <- as.data.frame(data)
summary(data)
class(data)
dias <- sapply(strsplit(archivosLista,"[.]"), `[`, 1)
dias <- as.numeric(dias)
dias <- rep(dias, each = 24)
meses <- sapply(strsplit(archivosLista,"[.]"), `[`, 2)
meses <- as.numeric(meses)
meses <- rep(meses, each = 24)
data$dia <- dias
data$mes <- meses
data$anio <- 2016
semana_mayo <-  c('Su', 'Mo', 'Tu', 'We', 'Th', 'Fr', 'Sa')
semana_abril <- c('Fr', 'Sa', 'Su', 'Mo', 'Tu', 'We', 'Th')
data$diaSemana <- 'dia de la semana'
for ( i in 1:dim(data)[1])
{
if(data[i, 'mes'] == 5)
{
data$diaSemana[i] <- semana_mayo[data$dia[i]%%7+1]
} else
{
data$diaSemana[i] <- semana_abril[data$dia[i]%%7+1]
}
}
table(data$diaSemana)
View(data)
library(jsonlite)
salidaResultados <- 'C:\\Users\\fou-f\\Desktop\\Consultoria\\python'
setwd(salidaResultados)# nos cambiamos de directorio
out <- 'C:\\Users\\fou-f\\Desktop\\Consultoria\\python\\datosdemanda'
entrada <- 'C:\\Users\\fou-f\\Desktop\\Consultoria\\python\\datosdemanda.zip'
archivosLista <- dir(out) #los archivos con los cuales trabajaremos
archivosLista <- sort(archivosLista)
archivosLista
datos <- lapply(1:length(archivosLista), FUN = function(i){
pathArchivo <- paste0(out,'\\', archivosLista[i])
archivoNojason  = fromJSON(pathArchivo)
archivoNojason
})
datos
data <- do.call("rbind", datos)
data$valorEnlace <- NULL
data <- apply(data, 2, function(x){
as.numeric(x)
})
data <- as.data.frame(data)
summary(data)
archivosLista
data <- apply(data, 2, function(x){
as.numeric(x)
})
data <- as.data.frame(data)
summary(data)
class(data)
dias <- sapply(strsplit(archivosLista,"[.]"), `[`, 1)
dias <- as.numeric(dias)
dias <- rep(dias, each = 24)
meses <- sapply(strsplit(archivosLista,"[.]"), `[`, 2)
meses <- as.numeric(meses)
meses <- rep(meses, each = 24)
data$dia <- dias
data$mes <- meses
data$anio <- 2016
semana_mayo <-  c('Su', 'Mo', 'Tu', 'We', 'Th', 'Fr', 'Sa')
semana_abril <- c('Fr', 'Sa', 'Su', 'Mo', 'Tu', 'We', 'Th')
data$diaSemana <- 'dia de la semana'
for ( i in 1:dim(data)[1])
{
if(data[i, 'mes'] == 5)
{
data$diaSemana[i] <- semana_mayo[data$dia[i]%%7+1]
} else
{
data$diaSemana[i] <- semana_abril[data$dia[i]%%7+1]
}
}
table(data$diaSemana)
summary(data)
130*4
130*8
10*130
130*8
